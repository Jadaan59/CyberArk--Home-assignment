# ğŸ” C/C++ Vulnerability Analyzer (LLM-Powered)

This CLI tool detects security vulnerabilities in C/C++ source files using a locally running large language model (LLM) via [Ollama](https://ollama.com). It reads your code, chunks it intelligently, and asks the model to identify potential security flaws â€” all running offline on your machine.

---

## âœ¨ Features

- ğŸ” Analyzes C/C++ files for common security issues
- ğŸ“¦ Works fully offline with local LLMs (e.g., `gemma3:4b`)
- ğŸ“š Processes large files in chunks for more stable results
- âœ… Simple CLI interface with clean output
- ğŸ’¬ Uses structured prompts for consistent and accurate answers

---

## ğŸ“¦ Requirements

- Python 3.8 or newer
- [Ollama](https://ollama.com) installed on your machine
- An Ollama-compatible model (e.g., `gemma3:4b`) downloaded locally

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git https://github.com/Jadaan59/CyberArk--Home-assignment.git
cd CyberArk--Home-assignment
```

### 2. Set Up Virtual Environment and python

#### ğŸ Don't Have Python Installed?

No worries â€” here's how to get it:

#### ğŸ”¸ Windows:
1. Go to: [https://www.python.org/downloads/windows/](https://www.python.org/downloads/windows/)
2. Download the latest version (Python 3.10+ recommended)
3. During installation, **make sure to check**: âœ… *"Add Python to PATH"*

#### ğŸ”¸ macOS:
You can install Python using [Homebrew](https://brew.sh):
```bash
brew install python
````

#### ğŸ”¸ Linux (optional):
```bash
sudo apt update
sudo apt install python3 python3-venv python3-pip
```
#### âœ… Create and Activate the Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate     # On Windows: venv\Scripts\activate
```

### 3. Install Python Dependencies

```bash
pip install ollama
```

### 4. Install Ollama

Follow instructions from [ollama.com/download](https://ollama.com/download) or use:

#### macOS (via Homebrew)
```bash
brew install ollama
```

#### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### Windows
Use the [official installer](https://ollama.com/download)

---

### 5. Download a Model (e.g., Gemma 4B)

```bash
ollama pull gemma3:4b
```
Or if you want smaller version 
```bash
ollama pull gemma3:1b
```
Or copy the attached Google Drive model
Make sure the model download finishes completely before continuing.

---

### 6. Start the Ollama Server

Open a separate terminal and run:

```bash
ollama serve
```

Leave this running in the background â€” it acts as a local API server.

---

### 7. Run the Analyzer

In your main terminal (with the virtual environment activated):

```bash
python analyzer.py path/to/your/code.c
```

Example:

```bash
python analyzer.py library.c
```

---

## ğŸ§  Sample Output

```text
ğŸ” Analyzing lines 1â€“80...

â€¼ï¸ Vulnerability detected at line 33:
Code: typedef struct Book { ... }
Issue: No bounds checking on strings (`title`, `author`)
Suggested Fix: Use strncpy, and validate input length before copy

â€¼ï¸ Vulnerability detected at line 101:
Code: int member_id;
Issue: Possible overflow with high user count
Suggested Fix: Add boundary check before assigning new member ID
```

---

## ğŸ›  Customization

You can edit `analyzer.py` to:

- Change the LLM model:
  ```python
  model = "gemma3:4b"
  ```
- Modify the prompt to suit your own rules
- Adjust chunk size for large files:
  ```python
  chunk_size = 80  # lines per chunk
  ```

---

## ğŸ—‚ Project Structure

```
.
â”œâ”€â”€ analyzer.py        # Main CLI script
â”œâ”€â”€ library.c          # Sample file for testing (optional)
â”œâ”€â”€ README.md          # Youâ€™re reading it
â”œâ”€â”€ venv/              # Python virtual environment (excluded from repo)
```
